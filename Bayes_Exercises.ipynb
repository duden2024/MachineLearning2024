{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izet53ioPAJC"
   },
   "source": [
    "# Bayes Exercises\n",
    "\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "In many ML libraries we use the terminology ```X``` for features and ```Y``` for labels/annotations\n",
    "or target values. X and Y are often numpy arrays. Such that Y is a column vector and X is a multidimensional array with:\n",
    "* the first dimension represents each sample\n",
    "* the remaining index dimensions represent the feature-values (these can be multi-dimensional in case we use images)\n",
    "\n",
    "The number of samples and the number of labels (i.e.: the number of entries in the first dimension of X and Y) must be the same.\n",
    "Acessing the 13-th feature vector and corresponding class in our training data:\n",
    "```python\n",
    "x13 = X[12, :] # index starts with 0\n",
    "y13 = Y[12]\n",
    "```\n",
    "\n",
    "Using logical indexing could come in to be really helpful here:\n",
    "```python\n",
    "    X[Y == 0,:]  # Fetch all features (from X) according to class label 0\n",
    "    X[Y == 1,:]  # Fetch all features (from X) according to class label 1\n",
    "```\n",
    "\n",
    "Also:\n",
    "* ```np.unique()```\n",
    "* ```np.sum()``` (have a look at the **axis** parameter)\n",
    "* ``` X.reshape()```\n",
    "\n",
    "could be helpful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kJmSdmsYPAJE"
   },
   "outputs": [],
   "source": [
    "# This cell generates your test values (they are the same as in the decision-tree exercise).\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "# Test features from previous exercise.\n",
    "#\n",
    "X_train = np.array([\n",
    "                    [2, 1, 3],       # Class 1: Red, Green, Blue\n",
    "                    [10, 30, 20],    # Class 2: Red, Green, Blue\n",
    "                    [1, 3, 2],       # Class 2: Red, Green, Blue\n",
    "                    [40, 20, 60]     # Class 1: Red, Green, Blue\n",
    "                 ], dtype=\"float32\")\n",
    "\n",
    "# Labels for each feature in X_train\n",
    "#\n",
    "Y_train = np.array([0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6p1JNxnPAJF"
   },
   "source": [
    "### Exercise 1: Implementation of calculating the prior values.\n",
    "\n",
    "* In this exercise your job is to implement the calculation prior values for each class.\n",
    "\n",
    "\n",
    "\n",
    "**Use the following stubs for your implementation:**\n",
    "\n",
    "\n",
    "```python\n",
    "def compute_priors(Y):\n",
    "    \"\"\" Compute the priors per class in Y.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y: np.array\n",
    "      A one dimensional numpy array containing class labels.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    A dictionary containing the priors per class.\n",
    "\n",
    "    Expected Output for X_train, Y_train\n",
    "    ----------\n",
    "    {0: 0.5, 1: 0.5}\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "PEUeFaAJPAJG"
   },
   "outputs": [],
   "source": [
    "def compute_priors(Y):\n",
    "    \"\"\" Compute the priors per class in Y.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y: np.array\n",
    "      A one dimensional numpy array containing class labels.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    A dictionary containing the priors per class.\n",
    "\n",
    "    Expected Output for X_train, Y_train\n",
    "    ----------\n",
    "    {0: 0.5, 1: 0.5}\n",
    "    \"\"\"\n",
    "    unique_labels = np.unique(Y)\n",
    "    priors = {}\n",
    "    for y_class in unique_labels:\n",
    "        priors[y_class] = Y[Y=y_class\n",
    "    return priors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = np.unique(Y_train)\n",
    "priors = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.int64(0): array([0, 0]), np.int64(1): array([1, 1])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for y_class in unique_labels:\n",
    "    priors[y_class] = len(Y_train[Y_train==y_class]) / len (Y\n",
    "priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([2, 2]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_xrvZWqPAJH"
   },
   "source": [
    "### Exercise 2: Implementation of calculating the pik values.\n",
    "\n",
    "**In this exercise your job is to implement the calculation of the probability values (pik) for each feature attribute under each class.**\n",
    "\n",
    "\n",
    "**Warning:**\n",
    "\n",
    "Be careful when computing the probabilities. We do not want larger images (more pixels and therefore more entries in a non-normalized histogram) to have a larger impact.\n",
    "Have a look at the features, you might have to normalize the individual feature vectors.\n",
    "\n",
    "**Use the following stubs for your implementation:**\n",
    "\n",
    "```python\n",
    "def compute_pik(X, Y):\n",
    "   \"\"\" Compute the probabilities per feature-dimension (pik).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.array\n",
    "      A two dimensional numpy array containing feature vectors (in rows) for each sample\n",
    "      in the training data.\n",
    "    Y: np.array\n",
    "      A one dimensional numpy array containing class labels.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    A dictionary containing the piks per class where keys are the class labels and values are the pik values.\n",
    "\n",
    "    Expected Output for X_train, Y_train\n",
    "    ----------\n",
    "    {0: array([0.33333333, 0.16666667, 0.5       ]),\n",
    "     1: array([0.16666667, 0.5       , 0.33333333])}\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ErNXBsAPAJQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDGnw7_bPAJQ"
   },
   "source": [
    "### Exercise 3: Implement the naive Bayes Classifier\n",
    "\n",
    "* Implement the Bayes classifier without using the log trick.\n",
    "* Implement the Bayes classifier with using the log trick.\n",
    "* Compare your results.\n",
    "\n",
    "Hint:\n",
    "* ```np.power()``` to compute $p_{ik}^{x_i}$ in a single instruction could be helpful\n",
    "* ```np.product()``` to compute the product of a list of values could be helpful\n",
    "\n",
    "**Use the following stubs for your implementation:**\n",
    "```python\n",
    "def classify_bayes(x, piks, priors):\n",
    "  \"\"\" Compute the posterior probability for a feature vector and perform classification according to naive Bayes.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  x: np.array\n",
    "    A one dimensional numpy array containing a single feature vector.\n",
    "  piks: dict\n",
    "    A dictionary of likelihoods as computed by compute_piks()\n",
    "  priors: dict\n",
    "    A dictionary of priors as computed by compute_priors()\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  Class label of the most probable class and a dictionary where the key is the class and the value is the posterior.\n",
    "\n",
    "  Expected Output for X_test[0,:]\n",
    "  ----------\n",
    "  (0, {0: 0.002057613168724279, 1: 6.430041152263372e-05})\n",
    "  \"\"\"\n",
    "  pass\n",
    "```\n",
    "\n",
    "**and**\n",
    "\n",
    "```python\n",
    "def classify_bayes_log(x, piks, priors):\n",
    "  \"\"\" Compute the log-posterior probability for a feature vector and perform classification according to naive Bayes.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  x: np.array\n",
    "    A one dimensional numpy array containing a single feature vector.\n",
    "  piks: dict\n",
    "    A dictionary of likelihoods as computed by compute_piks()\n",
    "  priors: dict\n",
    "    A dictionary of priors as computed by compute_priors()\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  Class label of the most probable class and a dictionary where the key is the class and the value is the posterior.\n",
    "\n",
    "  Expected Output for X_test[0,:]\n",
    "  ----------\n",
    "  (0, {0: -6.1862086239004945, 1: -9.65194452670022})\n",
    "  \"\"\"\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPZqRky_PAJR"
   },
   "outputs": [],
   "source": [
    "# Use this X_test to verify your implementation.\n",
    "#\n",
    "X_test = np.array([\n",
    "                    [5, 0, 0],    # x1\n",
    "                    [2, 1, 3],    # x2\n",
    "                    [2, 7, 4]     # x3\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wus04Y9PAJT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "71e3996440a4286f4b5430a3d4d1ae66fa68d894e5edac3334c7ebe5f98b7546"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
