{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izet53ioPAJC"
   },
   "source": [
    "# Bayes Exercises\n",
    "\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "In many ML libraries we use the terminology ```X``` for features and ```Y``` for labels/annotations\n",
    "or target values. X and Y are often numpy arrays. Such that Y is a column vector and X is a multidimensional array with:\n",
    "* the first dimension represents each sample\n",
    "* the remaining index dimensions represent the feature-values (these can be multi-dimensional in case we use images)\n",
    "\n",
    "The number of samples and the number of labels (i.e.: the number of entries in the first dimension of X and Y) must be the same.\n",
    "Acessing the 13-th feature vector and corresponding class in our training data:\n",
    "```python\n",
    "x13 = X[12, :] # index starts with 0\n",
    "y13 = Y[12]\n",
    "```\n",
    "\n",
    "Using logical indexing could come in to be really helpful here:\n",
    "```python\n",
    "    X[Y == 0,:]  # Fetch all features (from X) according to class label 0\n",
    "    X[Y == 1,:]  # Fetch all features (from X) according to class label 1\n",
    "```\n",
    "\n",
    "Also:\n",
    "* ```np.unique()```\n",
    "* ```np.sum()``` (have a look at the **axis** parameter)\n",
    "* ``` X.reshape()```\n",
    "\n",
    "could be helpful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kJmSdmsYPAJE"
   },
   "outputs": [],
   "source": [
    "# This cell generates your test values (they are the same as in the decision-tree exercise).\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "# Test features from previous exercise.\n",
    "#\n",
    "X_train = np.array([\n",
    "                    [2, 1, 3],       # Class 1: Red, Green, Blue\n",
    "                    [10, 30, 20],    # Class 2: Red, Green, Blue\n",
    "                    [1, 3, 2],       # Class 2: Red, Green, Blue\n",
    "                    [40, 20, 60]     # Class 1: Red, Green, Blue\n",
    "                 ], dtype=\"float32\")\n",
    "\n",
    "# Labels for each feature in X_train\n",
    "#\n",
    "Y_train = np.array([0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6p1JNxnPAJF"
   },
   "source": [
    "### Exercise 1: Implementation of calculating the prior values.\n",
    "\n",
    "* In this exercise your job is to implement the calculation prior values for each class.\n",
    "\n",
    "\n",
    "\n",
    "**Use the following stubs for your implementation:**\n",
    "\n",
    "\n",
    "```python\n",
    "def compute_priors(Y):\n",
    "    \"\"\" Compute the priors per class in Y.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y: np.array\n",
    "      A one dimensional numpy array containing class labels.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    A dictionary containing the priors per class.\n",
    "\n",
    "    Expected Output for X_train, Y_train\n",
    "    ----------\n",
    "    {0: 0.5, 1: 0.5}\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "PEUeFaAJPAJG"
   },
   "outputs": [],
   "source": [
    "def compute_priors(Y):\n",
    "    \"\"\" Compute the priors per class in Y.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y: np.array\n",
    "      A one dimensional numpy array containing class labels.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    A dictionary containing the priors per class.\n",
    "\n",
    "    Expected Output for X_train, Y_train\n",
    "    ----------\n",
    "    {0: 0.5, 1: 0.5}\n",
    "    \"\"\"\n",
    "    unique_labels = np.unique(Y)\n",
    "    priors = {}\n",
    "    for y_class in unique_labels:\n",
    "        priors[y_class] = len(Y[Y==y_class]) / len (Y)\n",
    "    return priors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = compute_priors(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.int64(0): 0.5, np.int64(1): 0.5}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_xrvZWqPAJH"
   },
   "source": [
    "### Exercise 2: Implementation of calculating the pik values.\n",
    "\n",
    "**In this exercise your job is to implement the calculation of the probability values (pik) for each feature attribute under each class.**\n",
    "\n",
    "\n",
    "**Warning:**\n",
    "\n",
    "Be careful when computing the probabilities. We do not want larger images (more pixels and therefore more entries in a non-normalized histogram) to have a larger impact.\n",
    "Have a look at the features, you might have to normalize the individual feature vectors.\n",
    "\n",
    "**Use the following stubs for your implementation:**\n",
    "\n",
    "```python\n",
    "def compute_pik(X, Y):\n",
    "   \"\"\" Compute the probabilities per feature-dimension (pik).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.array\n",
    "      A two dimensional numpy array containing feature vectors (in rows) for each sample\n",
    "      in the training data.\n",
    "    Y: np.array\n",
    "      A one dimensional numpy array containing class labels.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    A dictionary containing the piks per class where keys are the class labels and values are the pik values.\n",
    "\n",
    "    Expected Output for X_train, Y_train\n",
    "    ----------\n",
    "    {0: array([0.33333333, 0.16666667, 0.5       ]),\n",
    "     1: array([0.16666667, 0.5       , 0.33333333])}\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pik(X, Y, verbose = False):\n",
    "    \"\"\" Compute the probabilities per feature-dimension (pik).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.array\n",
    "      A two dimensional numpy array containing feature vectors (in rows) for each sample\n",
    "      in the training data.\n",
    "    Y: np.array\n",
    "      A one dimensional numpy array containing class labels.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    A dictionary containing the piks per class where keys are the class labels and values are the pik values.\n",
    "\n",
    "    Expected Output for X_train, Y_train\n",
    "    ----------\n",
    "    {0: array([0.33333333, 0.16666667, 0.5       ]),\n",
    "     1: array([0.16666667, 0.5       , 0.33333333])}\n",
    "    \"\"\"\n",
    "\n",
    "    # get unique labels\n",
    "    unique_labels = np.unique(Y)\n",
    "    # \n",
    "    pik = {}\n",
    "    \n",
    "    # go through each label\n",
    "    for label in unique_labels:\n",
    "        # get values for each label\n",
    "        x_label = X[Y == label,:]\n",
    "        # get draws for each run\n",
    "        draws_per_label = np.sum(x_label,  axis=1)\n",
    "        # normalize for each draw\n",
    "        normalized_total = np.zeros(X.shape[1])\n",
    "        for item, draws in zip(x_label, draws_per_label):\n",
    "            normalized = item / draws\n",
    "            normalized_total = normalized_total + normalized\n",
    "\n",
    "        # normalize over each label\n",
    "        normalized_total = normalized_total / np.sum(normalized_total)\n",
    "        pik[label] = normalized_total\n",
    "        \n",
    "    return pik\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "piks = compute_pik(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.int64(0): array([0.33333334, 0.16666667, 0.49999999]),\n",
       " np.int64(1): array([0.16666667, 0.49999999, 0.33333334])}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDGnw7_bPAJQ"
   },
   "source": [
    "### Exercise 3: Implement the naive Bayes Classifier\n",
    "\n",
    "* Implement the Bayes classifier without using the log trick.\n",
    "* Implement the Bayes classifier with using the log trick.\n",
    "* Compare your results.\n",
    "\n",
    "Hint:\n",
    "* ```np.power()``` to compute $p_{ik}^{x_i}$ in a single instruction could be helpful\n",
    "* ```np.product()``` to compute the product of a list of values could be helpful\n",
    "\n",
    "**Use the following stubs for your implementation:**\n",
    "```python\n",
    "def classify_bayes(x, piks, priors):\n",
    "  \"\"\" Compute the posterior probability for a feature vector and perform classification according to naive Bayes.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  x: np.array\n",
    "    A one dimensional numpy array containing a single feature vector.\n",
    "  piks: dict\n",
    "    A dictionary of likelihoods as computed by compute_piks()\n",
    "  priors: dict\n",
    "    A dictionary of priors as computed by compute_priors()\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  Class label of the most probable class and a dictionary where the key is the class and the value is the posterior.\n",
    "\n",
    "  Expected Output for X_test[0,:]\n",
    "  ----------\n",
    "  (0, {0: 0.002057613168724279, 1: 6.430041152263372e-05})\n",
    "  \"\"\"\n",
    "  pass\n",
    "```\n",
    "\n",
    "**and**\n",
    "\n",
    "```python\n",
    "def classify_bayes_log(x, piks, priors):\n",
    "  \"\"\" Compute the log-posterior probability for a feature vector and perform classification according to naive Bayes.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  x: np.array\n",
    "    A one dimensional numpy array containing a single feature vector.\n",
    "  piks: dict\n",
    "    A dictionary of likelihoods as computed by compute_piks()\n",
    "  priors: dict\n",
    "    A dictionary of priors as computed by compute_priors()\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  Class label of the most probable class and a dictionary where the key is the class and the value is the posterior.\n",
    "\n",
    "  Expected Output for X_test[0,:]\n",
    "  ----------\n",
    "  (0, {0: -6.1862086239004945, 1: -9.65194452670022})\n",
    "  \"\"\"\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_bayes(x, piks, priors):\n",
    "    \"\"\" Compute the posterior probability for a feature vector and perform classification according to naive Bayes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: np.array\n",
    "    A one dimensional numpy array containing a single feature vector.\n",
    "    piks: dict\n",
    "      A dictionary of likelihoods as computed by compute_piks()\n",
    "    priors: dict\n",
    "      A dictionary of priors as computed by compute_priors()\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Class label of the most probable class and a dictionary where the key is the class and the value is the posterior.\n",
    "    \n",
    "    Expected Output for X_test[0,:]\n",
    "    ----------\n",
    "    (0, {0: 0.002057613168724279, 1: 6.430041152263372e-05})\n",
    "    \"\"\"\n",
    "    probabilitiy = {}\n",
    "    for label in priors.keys():\n",
    "\n",
    "        probabilities[label] = priors[label] * np.prod(np.power(piks[label], X_test[0])piks[label])\n",
    "        \n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "piks: [0.33333334 0.16666667 0.49999999]\n",
      "piks products: 0.027777778191698906\n",
      "1\n",
      "piks: [0.16666667 0.49999999 0.33333334]\n",
      "piks products: 0.02777777819169891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{np.int64(0): np.float64(0.013888889095849453),\n",
       " np.int64(1): np.float64(0.013888889095849455)}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_bayes(X_test, piks, priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "id": "jPZqRky_PAJR"
   },
   "outputs": [],
   "source": [
    "# Use this X_test to verify your implementation.\n",
    "#\n",
    "X_test = np.array([\n",
    "                    [5, 0, 0],    # x1\n",
    "                    [2, 1, 3],    # x2\n",
    "                    [2, 7, 4]     # x3\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "_wus04Y9PAJT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.int64(0): 0.5, np.int64(1): 0.5}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.int64(0): array([0.33333334, 0.16666667, 0.49999999]),\n",
       " np.int64(1): array([0.16666667, 0.49999999, 0.33333334])}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([np.int64(0), np.int64(1)])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piks.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.int64(0): 0.5, np.int64(1): 0.5}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00411523, 1.        , 1.        ])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(piks[0], X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 2, 2])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "71e3996440a4286f4b5430a3d4d1ae66fa68d894e5edac3334c7ebe5f98b7546"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
